
Sparse stochastic inference for latent Dirichlet allocation

David Mimno mimno@cs.princeton.edu

Princeton U., Dept. of Computer Science, 35 Olden St., Princeton, NJ 08540, USA

Matthew D. Hoffman mdhoffma@cs.princeton.edu

Columbia U., Dept. of Statistics, Room 1005 SSW, MC 4690 1255 Amsterdam Ave. New York, NY 10027

David M. Blei blei@cs.princeton.edu

Princeton U., Dept. of Computer Science, 35 Olden St., Princeton, NJ 08540, USA

Abstract

We present a hybrid algorithm for Bayesian
topic models that combines the efficiency of
sparse Gibbs sampling with the scalability of
online stochastic inference. We used our algo-
rithm to analyze a corpus of 1.2 million books
(33 billion words) with thousands of topics.
Our approach reduces the bias of variational
inference and generalizes to many Bayesian
hidden-variable models.

1. Introduction

Topic models are hierarchical Bayesian models of doc-
ument collections (Blei et al., 2003). They can uncover
the main themes that pervade a corpus and then use
those themes to help organize, search, and explore the
documents. In topic modeling, a “topic” is a distri-
bution over a fixed vocabulary and each document ex-
hibits the topics with different proportions. Both the
topics and the topic proportions of documents are hid-
den variables. Inferring the conditional distribution of
these variables given an observed set of documents is
the central computational problem.

In this paper, we develop a posterior inference method
for topic modeling that can find large numbers of top-
ics in massive collections of documents. We demon-
strate our approach by analyzing a collection of 1.2
million out-of-copyright books, comprising 33 billion
observed words. Using our algorithm, we fit a topic
model to this corpus with thousands of topics. We il-
lustrate the most frequent words from several of these
topics in Table 1.

Appearing in Proceedings of the 29 th International Confer-
ence on Machine Learning, Edinburgh, Scotland, UK, 2012.
Copyright 2012 by the author(s)/owner(s).

Our algorithm builds on variational inference (Jor-
dan et al., 1999). In variational inference, we define
a parameterized family of distributions over the hid-
den structure—in this case topics and document-topic
proportions—and then optimize the parameters to find
a member of the family that is close to the poste-
rior. Traditional variational inference for topic model-
ing uses coordinate ascent. The algorithm alternates
between estimating document-topic proportions under
the current settings of the topics and re-estimating the
topics based on the estimated document proportions.
This requires multiple passes through an entire collec-
tion, which is not practical when working with very
large corpora.

Table 1. Randomly selected topics from a 2000-topic model
trained on a library of 1.2 million out-of-copyright books.

killed wounded sword slain arms military rifle wounds loss
human Plato Socrates universe philosophical minds ethics
inflammation affected abdomen ulcer circulation heart
ships fleet sea shore Admiral vessels land boats admiral
sister child tears pleasure daughters loves wont sigh warm
sentence clause syllable singular examples clauses syllables
provinces princes nations imperial possessions invasion
women Quebec Women Iroquois husbands thirty whom
steam engines power piston boilers plant supplied chimney
lines points direction planes Lines scale sections extending

Recently, Hoffman et al. (2010) introduced Online
LDA, a stochastic gradient optimization algorithm for
topic modeling. The algorithm repeatedly subsamples
a small set of documents from the collection and then
updates the topics from an analysis of the subsam-
ple. This method uses less memory than the standard
approach because we do not need to store topic pro-
portions for the full corpus. It also converges faster
because we update topics more frequently. However,
while it handles large corpora it does not scale to large



Sparse stochastic inference for latent Dirichlet allocation

numbers of topics.

Our algorithm builds on this method by using sam-
pling to introduce a second source of stochasticity into
the gradient. This approach lets us take advantage of
sparse computation, scaling sublinearly with the num-
ber of topics. Using this algorithm, we can fit topic
models to large collections with many topics.

2. Hybrid stochastic-MCMC inference

We model each of the D documents in a corpus as a
mixture of K topics. This topic model can be divided
into corpus-level global variables and document-level
local variables. The global variables are K topic-word
distributions ?1, ...,?K over the V -dimensional vocab-
ulary, each drawn from a Dirichlet prior with param-
eter ?. For a document d of length Nd, the local vari-
ables are (a) a distribution over topics ?d drawn from
a Dirichlet prior with parameter ? and (b) Nd token-
topic indicator variables zd1, ..., zdNd drawn from ?d.

Our goal is to estimate the posterior distribution of the
hidden variables given an observed corpus. We will use
variational inference. Unlike standard mean-field vari-
ational inference, but similar to Griffiths & Steyvers
(2004) and Teh et al. (2006), we will marginalize out
the topic proportions ?d. Thus we need to approxi-
mate the posterior over the topic assignments zd and
the topics ?1:K .

We will use a variational distribution of the form

q(z1, ...,zD,?1, ...,?K) =
?
d q(zd)

?
k q(?k). (1)

This factorization differs from the usual mean-field
family for topic models. Rather than defining a distri-
bution that factorizes over individual tokens, we treat
each document’s sequence of topic indicator variables
zd as a unit. As a result q(zd) will be a single dis-
tribution over the KNd possible topic configurations,
rather than a product of Nd distributions, each over
K possible values.

We now derive an algorithm that uses Gibbs sampling
to estimate variational expectations of the local vari-
ables and a stochastic natural gradient step to update
the variational distribution of global variables. A lower
bound on the marginal log probability of the observed
words given the hyperparameters is

log p(w|?, ?) ?
?
d

Eq log

[
p(zd|?)

?
i

?zdiwdi

]
(2)

+
?
k

Eq log p(?k|?) +H(q),

where H(q) denotes the entropy of q.

Following Bishop (2006), the optimal variational dis-
tribution over topic configurations for a document,
holding all other variational distributions fixed, is

q?(zd) ? exp{Eq(¬zd) [log p(zd|?)p(wd|zd,?)]} (3)

=
?(K?)

?(K?+Nd)

?
k

?(?+
?
i Izdi=k)

?(?)
(4)

×
?
i

expEq[log ?zdiwdi ]

where Ia=b is 1 if a = b and 0 otherwise, and ¬zd
denotes the set of all unobserved variables besides zd.
We can compute Eq. 4 for a specific topic configura-
tion zd, but we cannot tractably normalize it to get
the distribution q?(zd) over all K

Nd configurations.

The optimal variational distribution over topic-word
distributions, holding the other distributions fixed, is
the kernel of a Dirichlet distribution with parameters

?kw = ? +
?
d

?
i

Eq[Izdi=kIwdi=w]. (5)

This expression includes the expectation under q of
the number of tokens of type w assigned to topic k.
Computing this expectation would require evaluating
the intractable distribution q?(zd).

2.1. Online stochastic inference

We optimize the variational topic-word parameters
?kw using stochastic gradient ascent. Stochastic gra-
dient ascent iteratively updates parameters with noisy
estimates of the gradient. We obtain these noisy esti-
mates by subsampling the data (Sato, 2001; Hoffman
et al., 2010).

We first recast the variational objective in Eq. 2 as
a summation over per-document terms `d, so that the
full gradient with respect to ?k is the sum

?
d

?
??k

`d.
We can then generate a noisy approximation to this
full gradient by sampling a minibatch of documents
B and then scaling the sum of the document-specific
gradients to match the total size of the corpus,?

d

?

??k
`d = E

[
D

|B|
?
d?B

?

??k
`d

]
. (6)

(The expectation is with respect to the random sample
B.) Pushing the per-topic terms in Eq. 2 inside the
summation over documents and removing terms not
involving ?kw we obtain

`d =
?
w

(
Eq[Ndkw] +

1

D
(? ? ?kw)

)
Eq[log ?kw] (7)

+
1

D

(
log ?(

?
w ?kw)?

?
w

log ?(?kw)

)



Sparse stochastic inference for latent Dirichlet allocation

Algorithm 1 Algorithm for hybrid stochastic
variational-Gibbs inference.

for t ? 1, ...,? do
?t ?

(
1

t0+t

)?
sample minibatch B
for d ? B do

initialize z0d
discard B burn-in sweeps
for sample s ? 1, ..., S do

for token i ? 1, ..., Nd do
sample zsdi ? (?+Ndk)eEq [log ?kw]

end for
end for

end for
?tkw ? (1? ?t)?t?1kw + ?t

(
? + D|B|Nˆkw

)
end for

where Eq[Ndkw] =
?
i Eq[Izdi=kIwdi=w]. The gradient

of Eq. 7 with respect to the parameters ?k1, ..., ?kV
can be factored into the product of a matrix and a
vector. The matrix, which contains derivatives of the
digamma function, is the Fisher information matrix
for the topic parameters. Element w of the vector is

Eq[Ndkw] +
1

D
(? ? ?kw). (8)

Premultiplying the gradient of an objective function by
the inverse Fisher information matrix of the distribu-
tion being optimized (in our case the variational distri-
bution q) results in the natural gradient (Sato, 2001).
Since our gradient is the product of the Fisher informa-
tion matrix and a vector, the natural gradient is there-
fore simply Eq. 8 (Hoffman et al., 2010). Compared to
the standard Euclidean gradient, the natural gradient
offers both faster convergence (because it takes into
account the information geometry of the variational
distribution) and cheaper computation (because the
vector in Eq. 8 is a simple linear function).

2.2. MCMC within stochastic inference

We cannot evaluate the expectation in Eq. 8 because
we would have to consider a combinatorial number of
topic configurations zd. To use stochastic gradient as-
cent, however, we only need an approximation to this
expectation. We use Markov chain Monte Carlo to
sample topic configurations from q?(zd). We then use
the empirical average of these samples to estimate the
expectations needed for Eq. 8.

Gibbs sampling for a document starts with a random
initialization of the topic indicator variables zd. We
then iteratively resample the topic indicator at each
position from the conditional distribution over that

position given the remaining topic indicator variables:

q?(zdi = k|z\i) ? (?+
?
j 6=i Izj=k) exp{Eq[log ?kwdi ]},

(9)

where the expectation of the log probability of word
w given a topic k is ?(?kw) ? ?(

?
w? ?kw?). After B

burn-in sweeps, we begin saving sampled topic config-
urations. Once we have saved S samples {z}1,...,S , we
can define approximate sufficient statistics

Eq[Ndkw] ? Nˆkw = 1
S

?
s

?
d?B

?
i

Izsdi=kIwdi=w. (10)

Using MCMC estimates adds noise to our gradient,
but allows us to use a collapsed objective function that
does not represent document-topic proportions ?d. In
addition, an average over a finite set of samples pro-
vides a sparse estimate of the gradient: for many words
and topics, our estimate of Eq[Ndkw] will be zero.

2.3. Algorithm

We have defined a natural gradient and a method for
approximating the sufficient statistics of that gradient.
For a sequence of learning rates ?t = (t0 + t)

??, the
following update will lead to a stationary point:

?tkw ? ?t?1kw + ?t
D

|B|
?
d?B

(
Nˆdkw +

1

D
(? ? ?kw)

)

= (1? ?t)?t?1kw + ?t
(
? +

D

|B|
?
d?B

Nˆdkw

)
. (11)

This update results in Algorithm 1. Two implementa-
tion details that result in sparse computations can be
found in Appendix A. This online algorithm has the
important advantage over Online LDA of preserving
sparsity in the topic-word parameters, so that ?kw = ?
for most values of k and w. Sparsity increases the effi-
ciency of updates to ?k and of Gibbs sampling for zd.
Previous variational methods lead to dense updates to
KV topic parameters, making them expensive to ap-
ply to large vocabularies and large numbers of topics.
Our method, in contrast, is able to exploit the sparsity
exhibited by samples from the variational distribution
q?, resulting in much more efficient updates.

3. Related Work

This paper combines two sources of zero-mean noise in
constructing an approximate gradient for a variational
inference algorithm: subsampling of data, and Monte
Carlo inference. These sources of variance have been



Sparse stochastic inference for latent Dirichlet allocation

used individually in previous work. Stochastic approx-
imation EM (SAEM, Delyon et al., 1999) combines an
EM algorithm with a stochastic online inference proce-
dure. SAEM does not subsample data, but rather in-
terpolates between Monte Carlo estimates of the com-
plete data. Kuhn & Lavielle (2004) extend SAEM to
use MCMC estimates. Similarly, online EM (Cappe´ &
Moulines, 2009) sub-samples data but preserves stan-
dard inference procedures for local variables.

Standard collapsed Gibbs sampling uses multiple
sweeps over the entire corpus, representing topic-word
distributions using the topic-word assignment vari-
ables of the entire corpus except for the current token.
As a result, topic assignment variables must in theory
be sampled sequentially, although parallel approxima-
tions work well empirically (Asuncion et al., 2008). In
contrast, Algorithm 1 treats topic-word distributions
as a global variable distinct from the local token-topic
assignment variables, and so can parallelize trivially.

In this work we integrate over document-topic pro-
portions ?d within a variational algorithm. Collapsed
variational inference (Teh et al., 2006) also analytically
marginalizes over the topic proportions, but still main-
tains a fully factorized distribution over topic assign-
ments at each position. The method described here
does not restrict itself to such factored distributions,
and therefore reduces bias, but this reduction may be
offset by the bias we introduce when we initialize the
Gibbs chain.

4. Empirical Results

In this section we compare the sampled online algo-
rithm to two related online methods and measure the
effect of model parameters. We use a selection of met-
rics to evaluate models.

4.1. Evaluation

Held-out probability. A model that characterizes
the semantic structure of a corpus should place more
of its probability mass on sensible documents than
on random sequences of words. We can use this as-
sumption to compare different models by asking each
model to estimate the probability of a previously un-
seen document. A better model should, on average,
assign higher probability to real documents than a
lower-quality model. We evaluate held-out probabil-
ity using the left-to-right sequential sampling method
(Wallach et al., 2009; Buntine, 2009). For each trained
model we generate point estimates of the topic-word
probabilities p˜(w|k). We then process each document
by iterating through the tokens w1, ..., wNd . At each

position i we calculate the marginal probability

p˜(wi|w<i) =
?
k

p(zi = k|w<i, z<i, ?)p˜(wi|k). (12)

We then sample zi proportional to the terms of that
summation and continue to the next token.1 In order
to normalize for document lengths, we divide the sum
of the logs of the marginal probabilities by Nd.

Coherence. This metric measures the semantic
quality of a topic by approximating the experience of
a user viewing the W most probable words for the
topic (Mimno et al., 2011). It is related to point-wise
mutual information (Newman et al., 2010). Let D(w)
be the document frequencies for each word w, that is,
the number of documents containing one or more to-
kens of type w, and let D(w1, w2) be the number of
documents containing at least one token of w1 and of
w2. For each pair of words w1, w2 in the top W list,
we calculate the number of documents that contain at
least one token of the higher ranked word w1 that also
contain at least one token of the lower ranked word
w2:

C(W ) =
?
i

?
j<i

log
D(wi, wj) + ?

D(wj)
(13)

where ? is a small constant used to avoid log zero.
Values closer to zero indicate greater co-occurrence.
Unlike held-out probability, which reports scores for
held-out documents, coherence reports scores for indi-
vidual topics.

Wallclock time. Our goal is to train useful models
as efficiently as possible. In addition to model qual-
ity metrics, we are therefore also interested in total
computation time.

4.2. Comparison to Online VB

Our first corpus consists of 350,000 research articles
from three major journals: Science, Nature, and the
Proceedings of the National Academy of Sciences of
the USA. We use a vocabulary with 19,000 distinct
words, including selected multi-word terms. We train
models on 90% of the Science/Nature/PNAS corpus,
holding out the remaining documents for testing pur-
poses. We save topic-word parameters Nˆkw after
epochs consisting of 500,000 documents.

Sampled online variational Bayesian inference com-
pares well in terms of wallclock time to standard online
VB inference, particularly with respect to the number
of topics K. Figure 1 shows results comparing stan-
dard online VB inference to sampled online inference

1We set the document-topic hyperparameter ? = 0.1.



Sparse stochastic inference for latent Dirichlet allocation

Topics

Se
c

10

20

30

40

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll
lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll llllllllllllllllllllllllllllllllllllllllllllllllll
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll

llllllllllllllllllllllllllllllllllllllllllll

lllll
l
lll
lllllllllll

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll lllllllllllllllllllllllllllllllllllllllllllll
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll

llllllll
llllllllllll
lllllllllllllll
llllllllllll
lllllllllllll
llllllllllll
lllllllllll
llllllllllllllllllllllll lll

lllll
lllllll
llllllllllllll
llllllll
llllllll
l
l
lllllll
lllllllll
lllllllll
lllllll
llllllllll
lllllllll
lllllll
lllllllll
lllllllllll
llllllll
llllllllll
lllllll
ll
l
lllllll
lllllll

200 400 600 800 1000

Method
l VB
l Sampled

Figure 1. Comparison of seconds per mini-batch between
online variational Bayes (Hoffman et al., 2010) and sampled
online inference (this paper). Online VB is linear in K,
while sampled inference takes advantage of sparsity.

for K up to 1000. Each iteration consists of a mini-
batch of 100 documents. Standard online inference
takes time linear in K, while wallclock time for sam-
pled online inference grows more slowly.

We would like to know if there is a difference in the
quality of models trained through the hybrid sampled
variational algorithm and the online LDA algorithm.
We compare an implementation of Online LDA that
tries to be as close as possible to the sampled online
implementation, but using a dense VB update instead
of a sparse sampled update for the local variables. In
particular, the number of coordinate ascent steps in
standard VB is equal to the number of Gibbs sweeps
in the sampled algorithm.

Per-topic coherence for K = 200 is shown in Fig-
ure 2. Sampled online inference produces fewer very
poor topics. This difference is significant under a two-
sample t-test (p < 0.001) and does not decrease with
additional training epochs. Sampled online inference
also assigns greater held-out probability than Online
LDA for every test document, by a wide margin. We
evaluated several possible reasons for this difference in
performance. Held-out probability estimation can be
affected by evaluation-time smoothing parameter set-
tings, but we found both models were affected equally.
The log probability of a document is the sum of the
log probabilities of its words. It is possible that if
one model assigned very small probability to a hand-
ful of tokens, those words could significantly affect the
overall score, but the difference in log probability was
consistent across many tokens. The scale of parame-
ters might not be comparable, but as both methods
use the same learning schedule, the sum of the trained
parameters ?kw is nearly identical.

The main difference appears to be the entropy of the

Coherence

Al
go

rit
hm

SampOnline

VB

ll l

ll lll l lll ll l ll l ll l

?1200 ?1000 ?800 ?600 ?400 ?200

Figure 2. The new sampled online algorithm produces
fewer low-quality topics than Online LDA at K = 200.
Heldout log likelihood is much worse for Online LDA.

Coherence

Al
go

rit
hm

SampOnline

SMC

lll lll l lll lll lll ll l ll lll ll llll lll lll l lll lll ll lll ll l ll ll ll ll ll l ll ll l lll ll llll l ll

l llllll lll lllll ll l ll lllll ll lll lll ll ll ll l ll ll l l ll llll l lll ll ll

?1200 ?1000 ?800 ?600 ?400 ?200

HeldOut

Al
go

rit
hm

SampOnline

SMC

ll ll llllll l lll l l l ll l lllll ll llll l l l lll l lll ll l l l l lll l llll l l ll l l l l lllllll l l ll l l ll llll llllll lll l l l ll l llllll l lll l l l l ll lll lll ll ll l l lll l ll ll ll l lll l l

l lll l llll llll ll ll l llll l l l lll l l l lll l llll ll lll ll lll llll lllll l lll l l ll lll l lllllll l ll l ll l lll lllll ll ll l l l ll l lll ll llll ll lll lll ll llll ll l lll lll lll l l llll l l lll ll lll l lll l

?9.5 ?9.0 ?8.5 ?8.0 ?7.5 ?7.0

Figure 3. Sampled online inference performs better than
one pass of sequential Monte Carlo, after processing a com-
parable number of documents with K = 200.

topic distributions: the sampled-online algorithm pro-
duces less concentrated distributions (mean entropy
6.8 ± 0.46) than standard online LDA (mean entropy
6.0± 0.58). This result could indicate that coordinate
ascent over the local variables for Online LDA is not
converging.

4.3. Comparison to Sequential Monte Carlo

Sequential Monte Carlo is an online algorithm similar
to Gibbs sampling in that it represents topics using
sums over assignment variables (Ahmed et al., 2012).
A Gibbs sampler starts with a random initialization
for all hidden variables and sweeps repeatedly over the
entire data set, updating each variable given the cur-
rent value of all other variables. SMC samples values
for hidden variables in sequential order, conditioning
only on previously-seen variables. It is common to
keep multiple sampling states or “particles”, but this
process adds both computation and significant book-
keeping complexity. Ahmed et al. (2012) use a single
SMC state.

In order to compare SMC to the sampled online al-
gorithm, we ran 10 independent SMC samplers over
the Science/Nature/PNAS dataset, with documents



Sparse stochastic inference for latent Dirichlet allocation

ordered randomly. We also ran 10 independent sam-
pled trainers, stopping after a number of documents
had been sampled equivalent to the size of the corpus.
In order to make the comparison more fair, we allowed
the SMC sampler to sweep through each document the
same number of times as the sampled online algorithm,
but only the final topic configuration of a document
was available to the subsequent documents.2 Results
for K = 200 are shown in Figure 3. SMC has con-
sistently worse per-topic coherence and per-document
held-out log probability. The sampled online algo-
rithm in this paper differs from SMC in that the contri-
bution of local token-topic assignment variables decays
according to the learning rate schedule, so that more
recently sampled documents can have greater weight
than earlier documents. This decay allows sampled on-
line inference to “forget” its initial topics, unlike SMC,
which weights all documents equally.

4.4. Effect of parameter settings

Number of samples. In the inner loop of our algo-
rithm we initialize3 the topic indicator variables z for
a document and then perform several Gibbs sweeps.
In each sweep we resample the value of each topic in-
dicator variable in turn. We introduce bias when we
initialize, so we discard B “burn-in” sweeps and use
values of z saved after S additional sweeps to calculate
the gradient. Since performance is linear in the total
number of sweeps B + S, we want to find the smallest
number of sweeps that does not sacrifice performance.

We consider nine settings of the pair (B,S). Under
the first three settings we save one sweep and vary
the number of burn-in sweeps: (1,1), (2,1), (3,1). For
the second three settings we perform five sweeps, vary-
ing how many we discard: (2,3), (3,2), (4,1). The final
three settings fix B = S and consider larger total num-
bers of sweeps: (5,5), (10,10), (20,20). We evaluate
each setting after processing 500,000 documents.

Performance was similar across settings with the fol-
lowing exceptions, which were significant at p < 0.001
under a two-sample t-test. The two-sweep setting (1,1)
had better topic coherence but worse held-out proba-
bility than the all other settings. The (5,5) setting
had the best mean held-out probability, but it was
not significantly better than (10,10) and (20,20). The

2Note that SMC has an advantage. In the sampled
online algorithm we Gibbs sample each document within a
mini-batch independently, while in SMC, documents “see”
results from all previous documents.

3We initialize by sampling each token conditioned on
the topics of the previous tokens in the document:
p(zdi = k) ? (?+

?
j<i Izdj=k)p(wdi|k).

many-sweep settings (5,5), (10,10), (20,20) had worse
topic coherence than the other settings, with many vis-
ibly low-quality topics. These results suggest that 3–5
sweeps is sufficient.

Topic-word smoothing. Eq. 9 involves the func-
tion e?(x). This function approaches x ? 12 as x gets
large, but for values of x near 0, it is non-linear. For
example, e?(0.05) is 1034 times greater than e?(0.01). If
the values of topic parameters are in this range, a mi-
nuscule increase in the parameter for word w in topic
k can cause a profound change in the sampling distri-
bution for that word: all subsequent tokens of type w
will be assigned to topic k with probability near 1.0.

In general, the randomness introduced by sampling
topic assignments helps to avoid becoming trapped in
local maxima. When parameters are near zero, how-
ever, random decisions early in the inference process
risk becoming permanent. The topic-word smooth-
ing parameter ? can push parameter values away
from this explosive region. We measured coherence
for six settings of the topic-word hyperparameter ?,
{0.1, 0.2, 0.3, 0.4, 0.5, 0.6}. At ? = 0.1, a common value
for batch variational inference, many topics are visi-
bly nonsensical. Average coherence improves signifi-
cantly for each increasing value of ? ? {0.2, 0.3, 0.4}
(p < 0.001). There is no significant difference in aver-
age coherence for ? ? {0.4, 0.5, 0.6}.

Forgetting factors. We now consider the learning
rate ?t = (t0 + t)

?? and its relation to the corpus
size D. We fix ? = 0.6 and vary the offset param-
eter t0 ? {3000, 15000, 30000, 150000, 300000}, saving
topic parameters after five training epochs of 500,000
documents each. There was no significant difference
in average topic coherence.

The learning rate, however, is not the only factor that
determines the magnitude of parameter updates. Eq.
11 also includes the size of the corpus D. If the cor-
pus is larger, we will take larger steps, regardless of
the contents of the mini-batch. The offset parameter
t0 had no significant effect on coherence for the full
corpus, but it may have an effect if we also vary the
corpus size.

We simulate different size corpora by subsampling the
full data set. Results are shown in Figure 4 for models
trained on one half, one quarter, and one eighth of the
corpus. Each corpus is a subset of the next larger cor-
pus. In the smallest corpus (12.5%), the model with
t0 = 300000 is significantly worse than other settings
(p < 0.001). Otherwise, there is no significant differ-
ence in average topic coherence.



Sparse stochastic inference for latent Dirichlet allocation

t0

Co
he

re
nc

e

?1000

?800

?600

?400

?200
 12.5

l l

l l

l

l

l
l

l
ll
l

l

l
l

l

l

l

l

l

l

l

l
ll

l

l

l

l

3000 15000 30000 150000 300000

 25.0

l

l

l

ll

l

l

3000 15000 30000 150000 300000

 50.0

l l
l

l
l l

3000 15000 30000 150000 300000

100.0

l

l

3000 15000 30000 150000 300000

Figure 4. Topic quality is lowest for large values of t0, but only in small corpora. Panels represent the proportion of
training data used. Each panel shows coherence values for five K = 100 topic models with varying learning rates.

4.5. Scalability

Pre-1922 books. To demonstrate the scalability of
the method, we modeled a collection of 1.2 million out-
of-copyright books. Topic models are useful in char-
acterizing the contents of the corpus and supporting
browsing applications: even scanning titles for a col-
lection of this size is impossible for one person. Pre-
vious approaches to million-book digital libraries have
focused on keyword search and word frequency his-
tograms (Michel et al., 2011). Such methods do not
account for variability in meaning or context. There is
no guarantee that the words being counted match the
meaning assumed by the user. In contrast, an interface
based on a topic model could, for example, distinguish
uses of the word “strain” in immunology, mechanical
engineering, and cookery.

We divide each book into 10-page sections, resulting in
44 million “documents” with a vocabulary size of 216.
We trained models with K ? {100, 500, 1000, 2000}.
Randomly selected example topics are shown in Ta-
ble 1, illustrating the average level of topic quality.
Models are sparse: at K = 2000, less than 1% of the
2000 ·216 possible topic-word parameters are non-zero.
The algorithm scales well as K increases. The number
of milliseconds taken to process a sequence of 10,000
documents was similar for K = 1000 and 2000, despite
doubling the number of topics.

5. Conclusions

Stochastic online inference allows us to scale topic
modeling to large document sets. Sparse Gibbs sam-
pling allows us to scale to large numbers of topics.
The algorithm presented in this paper combines the
advantages of these two methods. As a result, models
can be trained on vast, open-ended corpora without
requiring access to vast computer clusters. If parallel
architectures are available, we can trivially parallelize
computation within each mini-batch. As this work is
related to the Online LDA algorithm of Hoffman et al.

(2010), extensions to that model are also applicable,
such as adaptive scheduling algorithms (Wahabzada &
Kersting, 2011). The use of MCMC within stochastic
variational inference reduces one source of bias in es-
timating local variables. Although we have focused on
text analysis applications, this hybrid method gener-
alizes to a broad class of Bayesian models.

Acknowledgments

John Langford, Iain Murray, Charles Sutton provided
helpful comments. Yahoo! and PICSciE provided
computational resources. DM is supported by a CRA
CI fellowship. MDH is supported by NSF ATM-
0934516, DOE DE-SC0002099, and IES R305D100017.
DMB is supported by ONR N00014-11-1-0651, NSF
CAREER 0745520, AFOSR FA9550-09-1-0668, the
Alfred P. Sloan foundation, and a grant from Google.

References

Ahmed, Amr, Aly, Mohamed, Gonzalez, Joseph,
Narayanamurthy, Shravan, and Smola, Alexander. Scal-
able inference in latent variable models. In WSDM, 2012.

Asuncion, Arthur, Smyth, Padhraic, and Welling, Max.
Asynchronous distributed learning of topic models. In
NIPS, 2008.

Bishop, Christopher M. Pattern Recognition and Machine
Learning. Springer, 2006.

Blei, David, Ng, Andrew, and Jordan, Michael. Latent
Dirichlet allocation. Journal of Machine Learning Re-
search, 3:993–1022, January 2003.

Buntine, Wray L. Estimating likelihoods for topic models.
In Asian Conference on Machine Learning, 2009.

Cappe´, Olivier and Moulines, Eric. Online EM algorithm
for latent data models. Journal of the Royal Statistical
Society Series B, 71(3):593–613, 2009.

Delyon, Bernard, Lavielle, Marc, and Moulines, Eric. Con-
vergence of a stochastic approximation version of the
EM algorithm. Annals of Statistics, 27(1):94–128, 1999.



Sparse stochastic inference for latent Dirichlet allocation

Griffiths, Thomas L. and Steyvers, Mark. Finding scientific
topics. PNAS, 101(suppl. 1):5228–5235, 2004.

Hoffman, Matthew, Blei, David, and Bach, Francis. Online
learning for latent dirichlet allocation. In NIPS, 2010.

Jordan, Michael, Ghahramani, Zoubin, Jaakkola, Tommi,
and Saul, Laurence. Introduction to variational methods
for graphical models. Machine Learning, 37:183–233,
1999.

Kuhn, Estelle and Lavielle, Marc. Coupling a stochastic
approximation version of EM with an MCMC procedure.
ESAIM: Probability and Statistics, 8:115–131, August
2004.

Michel, Jean-Baptiste, Shen, Yuan Kui, Aiden,
Aviva Presser, Veres, Adrian, Gray, Matthew K.,
Team, The Google Books, Pickett, Joseph P., Hoiberg,
Dale, Clancy, Dan, Norvig, Peter, Orwant, Jon, Pinker,
Steven, Nowak, Martin A., , and Aiden, Erez Lieber-
man. Quantitative analysis of culture using millions of
digitized books. Science, 311, 2011.

Mimno, David, Wallach, Hanna, Talley, Edmund, Leen-
ders, Miriam, and McCallum, Andrew. Optimizing se-
mantic coherence in topic models. In EMNLP, 2011.

Newman, David, Lau, Jey Han, Grieser, Karl, and Bald-
win, Timothy. Automatic evaluation of topic coherence.
In Human Language Technologies: The Annual Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, 2010.

Sato, M.A. Online model selection based on the variational
Bayes. Neural Computation, 13(7):1649–1681, 2001.

Teh, Yee-Whye, Newman, David, and Welling, Max. A col-
lapsed variational bayesian inference algorithm for latent
dirichlet allocation. In NIPS, 2006.

Wahabzada, Mirwaes and Kersting, Kristian. Larger resid-
uals, less work: Active document scheduling for latent
Dirichlet allocation. In ECML/PKDD, 2011.

Wallach, Hanna, Murray, Iain, Salakhutdinov, Ruslan, and
Mimno, David. Evaluation methods for topic models. In
ICML, 2009.

A. Sparse computation

Sparse sampling over topics. Sampling zsdi ?
(? + Ndk)e

Eq [log ?kw] requires calculating the normal-
izing constant Z = ?k(?+Ndk)eEq [log ?kw]. This cal-
culation can be accomplished in time much less than
O(k) if we can represent the topic-word parameters
?kw sparsely. The smoothing parameter ? can be fac-
tored out of Equation 11 as long as we assume that all
initial values ?0kw ? ?. Rearranging this equation to
separate the Dirichlet hyperparameter ?

?tkw ? ? + (1? ?t)
(
?t?1kw ? ?

)
+ ?t

D

|B|N
S
kw (14)

shows that we can define an alternative parameter
N˜ tkw = ?

t
kw ? ? that represents the “non-smoothing”

portion of the variational Dirichlet parameter, and ig-
nore the contribution of the smoothing parameter until
it is time to calculate expectations.

For any given w, it is likely that most values of N˜kw
will be zero. We can therefore rewrite the normalizing
constant as

Z =
?
k

?+Ndk

e?(V ?+N˜k?)

(
e?(?+N˜kw) ? e?(?)

)
+

?
k

?+Ndk

e?(V ?+N˜k?)
e?(?). (15)

The second summation does not depend on any word-
specific variables, and can therefore be calculated and
then updated incrementally as Ndk changes. The first
summation is non-zero only for k such that N˜kw > 0.

Sparse updates in the vocabulary. We expect
that a typical mini-batch will contain a small fraction
of the words in the vocabulary. Eq. 11, however, up-
dates N˜kw for all words, even words that do not occur
in the current mini-batch. Expanding the recursive
definition of N˜ tkw, and letting Nˆ

t
kw =

D
|B|N

S
kw,

N˜ tkw = ?tNˆ
t
kw + (1? ?t)

(
?t?1Nˆ t?1kw + (1? ?t?1)(...)

)
(16)

= ?tNˆ
t
kw + (1? ?t)?t?1Nˆ t?1kw + (1? ?t)(1? ?t?1)...

(17)

Dividing both sides by
?t
i=1(1? ?i),

N˜ tkw?t
i=1(1? ?i)

=
?tNˆ

t
kw?t

i=1(1? ?i)
+

?t?1Nˆ t?1kw?t?1
i=1(1? ?i)

(18)

+
?t?2Nˆ t?2kw?t?2
i=1(1? ?i)

+ ....

Defining a variable pit =
?t
i=1(1? ?i), the update be-

comes

N˜ tkw
pit

=
N˜ t?1kw
pit?1

+
?tNˆ

t
kw

pit
. (19)

This update is sparse: only elements with non-
zero ndw will be modified. To calculate the ex-

pectation of p(w|k), we compute ?
(
? + pit

Ntkw
pit

)
?

?
(
W? + pit

?
w
Ntkw
pit

)
.

The scale factor pit can become small after several hun-
dred mini-batches. We periodically “reset” this pa-

rameter by setting all stored values to N˜ tkw = pit
N˜tkw
pit

,
avoiding the possibility of numerical instability.


