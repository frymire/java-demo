
A Spectral Algorithm for Latent Dirichlet Allocation

Anima Anandkumar
University of California

Irvine, CA
a.anandkumar@uci.edu

Dean P. Foster
University of Pennsylvania

Philadelphia, PA
dean@foster.net

Daniel Hsu
Microsoft Research

Cambridge, MA
dahsu@microsoft.com

Sham M. Kakade
Microsoft Research

Cambridge, MA
skakade@microsoft.com

Yi-Kai Liu
National Institute of Standards and Technology∗

Gaithersburg, MD
yi-kai.liu@nist.gov

Abstract

Topic modeling is a generalization of clustering that posits that observations
(words in a document) are generated by multiple latent factors (topics), as op-
posed to just one. This increased representational power comes at the cost of
a more challenging unsupervised learning problem of estimating the topic-word
distributions when only words are observed, and the topics are hidden.
This work provides a simple and efficient learning procedure that is guaranteed to
recover the parameters for a wide class of topic models, including Latent Dirichlet
Allocation (LDA). For LDA, the procedure correctly recovers both the topic-word
distributions and the parameters of the Dirichlet prior over the topic mixtures,
using only trigram statistics (i.e., third order moments, which may be estimated
with documents containing just three words). The method, called Excess Corre-
lation Analysis, is based on a spectral decomposition of low-order moments via
two singular value decompositions (SVDs). Moreover, the algorithm is scalable,
since the SVDs are carried out only on k × k matrices, where k is the number
of latent factors (topics) and is typically much smaller than the dimension of the
observation (word) space.

1 Introduction

Topic models use latent variables to explain the observed (co-)occurrences of words in documents.
They posit that each document is associated with a (possibly sparse) mixture of active topics, and
that each word in the document is accounted for (in fact, generated) by one of these active topics.
In Latent Dirichlet Allocation (LDA) [1], a Dirichlet prior gives the distribution of active topics
in documents. LDA and related models possess a rich representational power because they allow
for documents to be comprised of words from several topics, rather than just a single topic. This
increased representational power comes at the cost of a more challenging unsupervised estimation
problem, when only the words are observed and the corresponding topics are hidden.

In practice, the most common unsupervised estimation procedures for topic models are based on
finding maximum likelihood estimates, through either local search or sampling based methods, e.g.,
Expectation-Maximization [2], Gibbs sampling [3], and variational approaches [4]. Another body
of tools is based on matrix factorization [5, 6]. For document modeling, a typical goal is to form
a sparse decomposition of a term by document matrix (which represents the word counts in each

∗Contributions to this work by NIST, an agency of the US government, are not subject to copyright laws.

1



document) into two parts: one which specifies the active topics in each document and the other
which specifies the distributions of words under each topic.

This work provides an alternative approach to parameter recovery based on the method of mo-
ments [7], which attempts to match the observed moments with those posited by the model. Our
approach does this efficiently through a particular decomposition of the low-order observable mo-
ments, which can be extracted using singular value decompositions (SVDs). This method is simple
and efficient to implement, and is guaranteed to recover the parameters of a wide class of topic
models, including the LDA model. We exploit exchangeability of the observed variables and, more
generally, the availability of multiple views drawn independently from the same hidden component.

1.1 Summary of contributions

We present an approach called Excess Correlation Analysis (ECA) based on the low-order (cross)
moments of observed variables. These observed variables are assumed to be exchangeable (and,
more generally, drawn from a multi-view model). ECA differs from Principal Component Analysis
and Canonical Correlation Analysis in that it is based on two singular value decompositions: the
first SVD whitens the data (based on the correlation between two observed variables) and the second
SVD uses higher-order moments (third- or fourth-order moments) to find directions which exhibit
non-Gaussianity, i.e., directions where the moments are in excess of those suggested by a Gaussian
distribution. The SVDs are performed only on k×k matrices, where k is the number of latent factors;
note that the number of latent factors (topics) k is typically much smaller than the dimension of the
observed space d (number of words).

The method is applicable to a wide class of latent variable models including exchangeable and multi-
view models. We first consider the class of exchangeable variables with independent latent factors.
We show that the (exact) low-order moments permit a decomposition that recovers the parameters for
model class, and that this decomposition can be computed using two SVD computations. We then
consider LDA and show that the same decomposition of a modified third-order moment correctly
recovers both the probability distribution of words under each topic, as well as the parameters of the
Dirichlet prior. We note that in order to estimate third-order moments in the LDA model, it suffices
for each document to contain at least three words.

While the methods described assume exact moments, it is straightforward to write down the ana-
logue “plug-in” estimators based on empirical moments from sampled data. We provide a simple
sample complexity analysis that shows that estimating the third-order moments is not as difficult as
it might naı̈vely seem since we only need a k × k matrix to be accurate.

Finally, we remark that the moment decomposition can also be obtained using other techniques,
including tensor decomposition methods and simultaneous matrix diagonalization methods. Some
preliminary experiments illustrating the efficacy of one such method is given in the appendix.

Omitted proofs, and additional results and discussion are provided in the full version of the paper [8].

1.2 Related work

Under the assumption that a single active topic occurs in each document, the work of [9] provides the
first provable guarantees for recovering the topic distributions (i.e., the distribution of words under
each topic), albeit with a rather stringent separation condition (where the words in each topic are
essentially non-overlapping). Understanding what separation conditions permit efficient learning
is a natural question; in the clustering literature, a line of work has focussed on understanding the
relationship between the separation of the mixture components and the complexity of learning. For
clustering, the first provable learnability result [10] was under a rather strong separation condition;
subsequent results relaxed [11–18] or removed these conditions [19–21]; roughly speaking, learning
under a weaker separation condition is more challenging, both computationally and statistically.
For the topic modeling problem in which only a single active topic is present per document, [22]
provides an algorithm for learning topics with no separation requirement, but under a certain full
rank assumption on the topic probability matrix.

For the case of LDA (where each document may be about multiple topics), the recent work of [23]
provides the first provable result under a natural separation condition. The condition requires that

2



each topic be associated with “anchor words” that only occur in documents about that topic. This
is a significantly milder assumption than the one in [9]. Under this assumption, [23] provide the
first provably correct algorithm for learning the topic distributions. Their work also justifies the
use of non-negative matrix (NMF) as a provable procedure for this problem (the original motivation
for NMF was as a topic modeling algorithm, though, prior to this work, formal guarantees as such
were rather limited). Furthermore, [23] provides results for certain correlated topic models. Our
approach makes further progress on this problem by relaxing the need for this separation condition
and establishing a much simpler procedure for parameter estimation.

The underlying approach we take is a certain diagonalization technique of the observed moments.
We know of at least three different settings which use this idea for parameter estimation.

The work in [24] uses eigenvector methods for parameter estimation in discrete Markov models
involving multinomial distributions. The idea has been extended to other discrete mixture models
such as discrete hidden Markov models (HMMs) and mixture models with a single active topic
in each document (see [22, 25, 26]). For such single topic models, the work in [22] demonstrates
the generality of the eigenvector method and the irrelevance of the noise model for the observations,
making it applicable to both discrete models like HMMs as well as certain Gaussian mixture models.

Another set of related techniques is the body of algebraic methods used for the problem of blind
source separation [27]. These approaches are tailored for independent source separation with addi-
tive noise (usually Gaussian) [28]. Much of the literature focuses on understanding the effects of
measurement noise, which often requires more sophisticated algebraic tools (typically, knowledge
of noise statistics or the availability of multiple views of the latent factors is not assumed). These
algebraic ideas are also used by [29, 30] for learning a linear transformation (in a noiseless setting)
and provides a different provably correct algorithm, based on a certain ascent algorithm (rather than
joint diagonalization approach, as in [27]), and a provably correct algorithm for the noisy case was
recently obtained by [31].

The underlying insight exploited by our method is the presence of exchangeable (or multi-view)
variables (e.g., multiple words in a document), which are drawn independently conditioned on the
same hidden state. This allows us to exploit ideas both from [24] and from [27]. In particular, we
show that the “topic” modeling problem exhibits a rather simple algebraic solution, where only two
SVDs suffice for parameter estimation.

Furthermore, the exchangeability assumption permits us to have an arbitrary noise model (rather
than an additive Gaussian noise, which is not appropriate for multinomial and other discrete distri-
butions). A key technical contribution is that we show how the basic diagonalization approach can
be adapted for Dirichlet models, through a rather careful construction. This construction bridges the
gap between the single topic models (as in [22, 24]) and the independent latent factors model.

More generally, the multi-view approach has been exploited in previous works for semi-supervised
learning and for learning mixtures of well-separated distributions (e.g., [16,18,32,33]). These previ-
ous works essentially use variants of canonical correlation analysis [34] between the two views. This
work follows [22] in showing that having a third view of the data permits rather simple estimation
procedures with guaranteed parameter recovery.

2 The independent latent factors and LDA models

Let h = (h1, h2, . . . , hk) ∈ Rk be a random vector specifying the latent factors (i.e., the hidden
state) of a model, where hi is the value of the i-th factor. Consider a sequence of exchangeable ran-
dom vectors x1, x2, x3, x4, . . . ∈ Rd, which we take to be the observed variables. Assume through-
out that d ≥ k; that x1, x2, x3, x4, . . . ∈ Rd are conditionally independent given h. Furthermore,
assume there exists a matrix O ∈ Rd×k such that

E[xv|h] = Oh

for each v ∈ {1, 2, 3, . . . }. Throughout, we assume the following condition.
Condition 2.1. O has full column rank.

This is a mild assumption, which allows for identifiability of the columns of O. The goal is to
estimate the matrix O, sometimes referred to as the topic matrix. Note that at this stage, we have not
made any assumptions on the noise model; it need not be additive nor even independent of h.

3



2.1 Independent latent factors model

In the independent latent factors model, we assume h has a product distribution, i.e., h1, h2, . . . , hk
are independent. Two important examples of this setting are as follows.

Multiple mixtures of Gaussians: Suppose xv = Oh + η, where η is Gaussian noise and h is a
binary vector (under a product distribution). Here, the i-th column Oi can be considered to be the
mean of the i-th Gaussian component. This generalizes the classic mixture of k Gaussians, as the
model now permits any number of Gaussians to be responsible for generating the hidden state (i.e.,
h is permitted to be any of the 2k vectors on the hypercube, while in the classic mixture problem,
only one component is responsible). We may also allow η to be heteroskedastic (i.e., the noise may
depend on h, provided the linearity assumption E[xv|h] = Oh holds).

Multiple mixtures of Poissons: Suppose [Oh]j specifies the Poisson rate of counts for [xv]j . For
example, xv could be a vector of word counts in the v-th sentence of a document. Here, O would be
a matrix with positive entries, and hi would scale the rate at which topic i generates words in a sen-
tence (as specified by the i-th column of O). The linearity assumption is satisfied as E[xv|h] = Oh
(note the noise is not additive in this case). Here, multiple topics may be responsible for generating
the words in each sentence. This model provides a natural variant of LDA, where the distribution
over h is a product distribution (while in LDA, h is a probability vector).

2.2 The Dirichlet model

Now suppose the hidden state h is a distribution itself, with a density specified by the Dirichlet
distribution with parameter α ∈ Rk>0 (α is a strictly positive real vector). We often think of h as a
distribution over topics. Precisely, the density of h ∈ ∆k−1 (where the probability simplex ∆k−1

denotes the set of possible distributions over k outcomes) is specified by:

pα(h) :=
1

Z(α)

k∏
i=1

hαi−1
i

where Z(α) :=
∏k

i=1 Γ(αi)

Γ(α0) and α0 := α1 + α2 + · · ·+ αk. Intuitively, α0 (the sum of the “pseudo-
counts”) characterizes the concentration of the distribution. As α0 → 0, the distribution degenerates
to one over pure topics (i.e., the limiting density is one in which, almost surely, exactly one coordi-
nate of h is 1, and the rest are 0).

Latent Dirichlet Allocation: LDA makes the further assumption that each random variable
x1, x2, x3, . . . takes on discrete values out of d outcomes (e.g., xv represents what the v-th word
in a document is, so d represents the number of words in the language). The i-th column Oi of
O is a probability vector representing the distribution over words for the i-th topic. The sampling
process for a document is as follows. First, the topic mixture h is drawn from the Dirichlet dis-
tribution. Then, the v-th word in the document (for v = 1, 2, . . . ) is generated by: (i) drawing
t ∈ [k] := {1, 2, . . . k} according to the discrete distribution specified by h, then (ii) drawing xv
according to the discrete distribution specified by Ot (the t-th column of O). Note that xv is in-
dependent of h given t. For this model to fit in our setting, we use the “one-hot” encoding for xv
from [22]: xv ∈ {0, 1}d with [xv]j = 1 iff the v-th word in the document is the j-th word in the
vocabulary. Observe that

E[xv|h] =

k∑
i=1

Pr[t = i|h] · E[xv|t = i, h] =

k∑
i=1

hi ·Oi = Oh

as required. Again, note that the noise model is not additive.

3 Excess Correlation Analysis (ECA)

We now present efficient algorithms for exactly recovering O from low-order moments of the ob-
served variables. The algorithm is based on two singular value decompositions: the first SVD
whitens the data (based on the correlation between two variables), and the second SVD is carried

4



Algorithm 1 ECA, with skewed factors
Input: vector θ ∈ Rk; the moments Pairs and Triples.

1. Dimensionality reduction: Find a matrix U ∈ Rd×k such that

range(U) = range(Pairs).

(See Remark 1 for a fast procedure.)
2. Whiten: Find V ∈ Rk×k so V >(U> PairsU)V is the k × k identity matrix. Set:

W = UV.

3. SVD: Let Ξ be the set of left singular vectors of

W> Triples(Wθ)W

corresponding to non-repeated singular values (i.e., singluar values with multiplicity
one).

4. Reconstruct: Return the set

Ô := {(W+)>ξ : ξ ∈ Ξ}.

out on higher-order moments. We start with the case of independent factors, as these algorithms
make the basic diagonalization approach clear.

Throughout, we use A+ to denote the Moore-Penrose pseudo-inverse.

3.1 Independent and skewed latent factors

Define the following moments:
µ := E[x1], Pairs := E[(x1 − µ)⊗ (x2 − µ)], Triples := E[(x1 − µ)⊗ (x2 − µ)⊗ (x3 − µ)]

(here ⊗ denotes the tensor product, so µ ∈ Rd, Pairs ∈ Rd×d, and Triples ∈ Rd×d×d). It is
convenient to project Triples to matrices as follows:

Triples(η) := E[(x1 − µ)(x2 − µ)>〈η, x3 − µ〉].
Roughly speaking, we can think of Triples(η) as a re-weighting of a cross covariance (by 〈η, x3 −
µ〉).
Note that the matrix O is only identifiable up to permutation and scaling of columns. To see the
latter, observe the distribution of any xv is unaltered if, for any i ∈ [k], we multiply the i-th column
of O by a scalar c 6= 0 and divide the variable hi by the same scalar c. Without further assumptions,
we can only hope to recover a certain canonical form of O, defined as follows.
Definition 1 (Canonical form). We say O is in a canonical form (relative to h) if, for each i ∈ [k],

σ2
i := E[(hi − E[hi])

2] = 1.

The transformation O ← O diag(σ1, σ2, . . . , σk) (and a rescaling of h) places O in canonical form
relative to h, and the distribution over x1, x2, x3, . . . is unaltered. In canonical form, O is unique up
to a signed column permutation.

Let µi,p := E[(hi − E[hi])
p] denote the p-th central moment of hi, so the variance and skewness of

hi are given by σ2
i := µi,2 and γi := µi,3/σ

3
i . The first result considers the case when the skewness

is non-zero.
Theorem 3.1 (Independent and skewed factors). Assume Condition 2.1 and σ2

i > 0 for each i ∈ [k].
Under the independent latent factor model, the following hold.

• No False Positives: For all θ ∈ Rk, Algorithm 1 returns a subset of the columns of O, in
canonical form up to sign.

• Exact Recovery: Assume γi 6= 0 for each i ∈ [k]. If θ ∈ Rk is drawn uniformly at random
from the unit sphere Sk−1, then with probability 1, Algorithm 1 returns all columns of O,
in canonical form up to sign.

5



The proof of this theorem relies on the following lemma.
Lemma 3.1 (Independent latent factors moments). Under the independent latent factor model,

Pairs =

k∑
i=1

σ2
i Oi ⊗Oi = O diag(σ2

1 , σ
2
2 , . . . , σ

2
k)O>,

Triples =

k∑
i=1

µi,3 Oi ⊗Oi ⊗Oi, Triples(η) = O diag(O>η) diag(µ1,3, µ2,3, . . . , µk,3)O>.

Proof. The model assumption E[xv|h] = Oh implies µ = OE[h]. Therefore E[(xv − µ)|h] =
O(h − E[h]). Using the conditional independence of x1 and x2 given h, and the fact that h has a
product distribution,

Pairs = E[(x1 − µ)⊗ (x2 − µ)] = E[E[(x1 − µ)|h]⊗ E[(x2 − µ)|h]]

= OE[(h− E[h])⊗ (h− E[h])]O> = O diag(σ2
1 , σ

2
2 , . . . , σ

2
k)O>.

An analogous argument gives the claims for Triples and Triples(η).

Proof of Theorem 3.1. Assume O is in canonical form with respect to h. By Condition 2.1,
U> PairsU ∈ Rk×k is full rank and hence positive definite. Thus the whitening step is pos-
sible, and M := W>O is orthogonal. Observe that W> Triples(Wθ)W = MDM>, where
D := diag(M>θ) diag(γ1, γ2, . . . , γk). Since M is orthogonal, the above is an eigendecompo-
sition of W> Triples(Wθ)W , and hence the set of left singular vectors corresponding to non-
repeated singular values are uniquely defined up to sign. Each such singular vector ξ is of the
form siMei = siW

>Oei = siW
>Oi for some i ∈ [k] and si ∈ {±1}, so (W+)>ξ =

siW (W>W )−1W>Oi = siOi (because range(W ) = range(U) = range(O)).

If θ is drawn uniformly at random from Sk−1, then so is M>θ. In this case, almost surely, the
diagonal entries of D are unique (provided that each γi 6= 0), and hence every singular value of
W> Triples(Wθ)W is non-repeated.

Remark 1 (Finding range(Pairs) efficiently). Let Θ ∈ Rd×k be a random matrix with entries sam-
pled independently from the standard normal distribution, and set U := Pairs Θ. Then with proba-
bility 1, range(U) = range(Pairs).

It is easy to extend Algorithm 1 to kurtotic sources where κi := (µi,4/σ
4
i )− 3 6= 0 for each i ∈ [k],

simply by using fourth-order cumulants in places of Triples(η). The details are given in the full
version of the paper.

3.2 Latent Dirichlet Allocation

Now we turn to LDA where h has a Dirichlet density. Even though the distribution on h is propor-
tional to the product hα1−1

1 hα2−1
2 · · ·hαk−1

k , the hi are not independent because h is constrained to
live in the simplex. These mild dependencies suggest using a certain correction of the moments with
ECA.

We assume α0 is known. Knowledge of α0 = α1 + α2 + · · · + αk is significantly weaker than
having full knowledge of the entire parameter vector α = (α1, α2, . . . , αk). A common practice
is to specify the entire parameter vector α in a homogeneous manner, with each component being
identical (see [35]). Here, we need only specify the sum, which allows for arbitrary inhomogeneity
in the prior.

Denote the mean and a modified second moment by

µ = E[x1], Pairsα0
:= E[x1x

>
2 ]− α0

α0 + 1
µµ>,

and a modified third moment as

Triplesα0
(η) := E[x1x

>
2 〈η, x3〉]−

α0

α0 + 2

(
E[x1x

>
2 ]ηµ> + µη>E[x1x

>
2 ] + 〈η, µ〉E[x1x

>
2 ]
)

+
2α2

0

(α0 + 2)(α0 + 1)
〈η, µ〉µµ>.

6



Algorithm 2 ECA for Latent Dirichlet Allocation
Input: vector θ ∈ Rk; the modified moments Pairsα0 and Triplesα0

.
1–3. Execute steps 1–3 of Algorithm 1 with Pairsα0

and Triplesα0
in place of Pairs and

Triples.
4. Reconstruct and normalize: Return the set

Ô :=

{
(W+)>ξ

~1>(W+)>ξ
: ξ ∈ Ξ

}
where ~1 ∈ Rd is a vector of all ones.

Remark 2 (Central vs. non-central moments). In the limit as α0 → 0, the Dirichlet model degen-
erates so that, with probability 1, only one coordinate of h equals 1 and the rest are 0 (i.e., each
document is about just one topic). In this case, the modified moments tend to the raw (cross) mo-
ments:

lim
α0→0

Pairsα0 = E[x1 ⊗ x2], lim
α0→0

Triplesα0
= E[x1 ⊗ x2 ⊗ x3].

Note that the one-hot encoding of words in xv implies that

E[x1⊗x2] =
∑

1≤i,j≤d

Pr[x1 = ei, x2 = ej ] ei⊗ej =
∑

1≤i,j≤d

Pr[1st word = i, 2nd word = j] ei⊗ej ,

(and a similar expression holds for E[x1 ⊗ x2 ⊗ x3]), so these raw moments in the limit α0 → 0 are
precisely the joint probabilitiy tables of words across all documents.

At the other extreme α0 →∞, the modified moments tend to the central moments:

lim
α0→∞

Pairsα0
= E[(x1−µ)⊗ (x2−µ)], lim

α0→∞
Triplesα0

= E[(x1−µ)⊗ (x2−µ)⊗ (x3−µ)]

(to see this, expand the central moment and use exchangeability: E[x1x
>
2 ] = E[x2x

>
3 ] = E[x1x

>
3 ]).

Our main result here shows that ECA recovers both the topic matrix O, up to a permutation of the
columns (where each column represents a probability distribution over words for a given topic) and
the parameter vector α, using only knowledge of α0 (which, as discussed earlier, is a significantly
less restrictive assumption than tuning the entire parameter vector).
Theorem 3.2 (Latent Dirichlet Allocation). Assume Condition 2.1 holds. Under the LDA model,
the following hold.

• No False Positives: For all θ ∈ Rk, Algorithm 2 returns a subset of the columns of O.

• Topic Recovery: If θ ∈ Rk is drawn uniformly at random from the unit sphere Sk−1, then
with probability 1, Algorithm 2 returns all columns of O.

• Parameter Recovery: The Dirichlet parameter α satisfies α = α0(α0 +

1)O+ Pairsα0(O+)>~1, where ~1 ∈ Rk is a vector of all ones.

The proof relies on the following lemma.
Lemma 3.2 (LDA moments). Under the LDA model,

Pairsα0 =
1

(α0 + 1)α0
O diag(α)O>,

Triplesα0
(η) =

2

(α0 + 2)(α0 + 1)α0
O diag(O>η) diag(α)O>.

The proof of Lemma 3.2 is similar to that of Lemma 3.1, except here we must use the specific
properties of the Dirichlet distribution to show that the corrections to the raw (cross) moments have
the desired effect.

Proof of Theorem 3.2. Note that with the rescaling Õ := 1√
(α0+1)α0

O diag(
√
α1,
√
α2, . . . ,

√
αk),

we have that Pairsα0 = ÕÕ>. This is akin to Õ being in canonical form as per the skewed factor

7



model of Theorem 3.1. Now the proof of the first two claims is the same as that of Theorem 3.1; the
only modification is that we simply normalize the output of Algorithm 1. Finally, observe that claim
for estimating α holds due to the functional form of Pairsα0

.

Remark 3 (Limiting behaviors). ECA seamlessly interpolates between the single topic model (α0 →
0) of [22] and the skewness-based ECA, Algorithm 1 (α0 →∞).

4 Discussion

4.1 Sample complexity

It is straightforward to derive a “plug-in” variant of Algorithm 2 based on empirical moments rather
than exact population moments. The empirical moments are formed using the word co-occurrence
statistics for documents in a corpus. The following theorem shows that the empirical version of ECA
returns accurate estimates of the topics. The details and proof are left to the full version of the paper.
Theorem 4.1 (Sample complexity for LDA). There exist universal constants C1, C2 > 0 such that
the following hold. Let pmin = mini

αi

α0
and let σk(O) denote the smallest (non-zero) singular

value of O. Suppose that we obtain N ≥ C1 · ((α0 + 1)/(pminσk(O)2))2 independent samples
of x1, x2, x3 in the LDA model, which are used to form empirical moments P̂airsα0

and T̂riplesα0
.

With high probability, the plug-in variant of Algorithm 2 returns a set {Ô1, Ô2, . . . Ôk} such that,
for some permutation σ of [k],

‖Oi − Ôσ(i)‖2 ≤ C2 ·
(α0 + 1)2k3

p2
minσk(O)3

√
N
, ∀i ∈ [k].

4.2 Alternative decomposition methods

Algorithm 1 is a theoretically efficient and simple-to-state method for obtaining the desired decom-
position of the tensor Triples =

∑k
i=1 µi,3Oi ⊗ Oi ⊗ Oi (a similar tensor form for Triplesα0

in
the case of LDA can also be given). However, in practice the method is not particularly stable, due
to the use of internal randomization to guarantee strict separation of singular values. It should be
noted that there are other methods in the literature for obtaining these decompositions, for instance,
methods based on simultaneous diagonalizations of matrices [36] as well as direct tensor decom-
position methods [37]; and that these methods can be significantly more stable than Algorithm 1.
In particular, very recent work in [37] shows that the structure revealed in Lemmas 3.1 and 3.2 can
be exploited to derive very efficient estimation algorithms for all the models considered here (and
others) based on a tensor power iteration. We have used a simplified version of this tensor power it-
eration in preliminary experiments for estimating topic models, and found the results (Appendix A)
to be very encouraging, especially due to the speed and robustness of the algorithm.

Acknowledgements

We thank Kamalika Chaudhuri, Adam Kalai, Percy Liang, Chris Meek, David Sontag, and Tong
Zhang for many invaluable insights. We also give warm thanks to Rong Ge for sharing preliminary
results (in [23]) and early insights into this problem with us. Part of this work was completed while
all authors were at Microsoft Research New England. AA is supported in part by the NSF Award
CCF-1219234, AFOSR Award FA9550-10-1-0310 and the ARO Award W911NF-12-1-0404.

References
[1] David M. Blei, Andrew Ng, and Michael Jordan. Latent dirichlet allocation. JMLR, 3:993–1022, 2003.

[2] R. A. Redner and H. F. Walker. Mixture densities, maximum likelihood and the EM algorithm. SIAM
Review, 26(2):195–239, 1984.

[3] A. Asuncion, P. Smyth, M. Welling, D. Newman, I. Porteous, and S. Triglia. Distributed gibbs sam-
pling for latent variable models. In Scaling Up Machine Learning: Parallel and Distributed Approaches.
Cambridge Univ Pr, 2011.

[4] M.D. Hoffman, D.M. Blei, and F. Bach. Online learning for latent dirichlet allocation. In NIPS, 2010.

8



[5] Thomas Hofmann. Probilistic latent semantic analysis. In UAI, 1999.

[6] Daniel D. Lee and H. Sebastian Seung. Learning the parts of objects by nonnegative matrix factorization.
Nature, 401, 1999.

[7] K. Pearson. Contributions to the mathematical theory of evolution. Phil. Trans. of the Royal Society,
London, A., 1894.

[8] A. Anandkumar, D. P. Foster, D. Hsu, S. M. Kakade, and Y.-K. Liu. Two svds suffice: spectral decompo-
sitions for probabilistic topic models and latent dirichlet allocation, 2012. arXiv:1204.6703.

[9] Christos H. Papadimitriou, Prabhakar Raghavan, Hisao Tamaki, and Santosh Vempala. Latent semantic
indexing: A probabilistic analysis. J. Comput. Syst. Sci., 61(2), 2000.

[10] S. Dasgupta. Learning mixutres of Gaussians. In FOCS, 1999.

[11] S. Dasgupta and L. Schulman. A two-round variant of em for gaussian mixtures. In UAI, 2000.

[12] S. Arora and R. Kannan. Learning mixtures of arbitrary Gaussians. In STOC, 2001.

[13] S. Vempala and G. Wang. A spectral algorithm for learning mixtures of distributions. In FOCS, 2002.

[14] R. Kannan, H. Salmasian, and S. Vempala. The spectral method for general mixture models. In COLT,
2005.

[15] D. Achlioptas and F. McSherry. On spectral learning of mixtures of distributions. In COLT, 2005.

[16] K. Chaudhuri and S. Rao. Learning mixtures of product distributions using correlations and independence.
In COLT, 2008.

[17] S. C. Brubaker and S. Vempala. Isotropic PCA and affine-invariant clustering. In FOCS, 2008.

[18] K. Chaudhuri, S. M. Kakade, K. Livescu, and K. Sridharan. Multi-view clustering via canonical correla-
tion analysis. In ICML, 2009.

[19] A. T. Kalai, A. Moitra, and G. Valiant. Efficiently learning mixtures of two Gaussians. In STOC, 2010.

[20] M. Belkin and K. Sinha. Polynomial learning of distribution families. In FOCS, 2010.

[21] A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of Gaussians. In FOCS, 2010.

[22] A. Anandkumar, D. Hsu, and S. M. Kakade. A method of moments for mixture models and hidden
markov models. In COLT, 2012.

[23] S. Arora, , R. Ge, and A. Moitra. Learning topic models — going beyond svd. In FOCS, 2012.

[24] J. T. Chang. Full reconstruction of Markov models on evolutionary trees: Identifiability and consistency.
Mathematical Biosciences, 137:51–73, 1996.

[25] E. Mossel and S. Roch. Learning nonsingular phylogenies and hidden Markov models. Annals of Applied
Probability, 16(2):583–614, 2006.

[26] D. Hsu, S. M. Kakade, and T. Zhang. A spectral algorithm for learning hidden Markov models. In COLT,
2009.

[27] Jean-Franois Cardoso and Pierre Comon. Independent component analysis, a survey of some algebraic
methods. In IEEE International Symposium on Circuits and Systems, pages 93–96, 1996.

[28] P. Comon and C. Jutten. Handbook of Blind Source Separation: Independent Component Analysis and
Applications. Academic Press. Elsevier, 2010.

[29] Alan M. Frieze, Mark Jerrum, and Ravi Kannan. Learning linear transformations. In FOCS, 1996.

[30] P. Q. Nguyen and O. Regev. Learning a parallelepiped: Cryptanalysis of GGH and NTRU signatures.
Journal of Cryptology, 22(2):139–160, 2009.

[31] S. Arora, R. Ge, A. Moitra, and S. Sachdeva. Provable ICA with unknown Gaussian noise, and implica-
tions for Gaussian mixtures and autoencoders. In NIPS, 2012.

[32] R. Ando and T. Zhang. Two-view feature generation model for semi-supervised learning. In ICML, 2007.

[33] Sham M. Kakade and Dean P. Foster. Multi-view regression via canonical correlation analysis. In COLT,
2007.

[34] H. Hotelling. The most predictable criterion. Journal of Educational Psychology, 26(2):139–142, 1935.

[35] Mark Steyvers and Tom Griffiths. Probabilistic topic models. In T. Landauer, D. Mcnamara, S. Dennis,
and W. Kintsch, editors, Latent Semantic Analysis: A Road to Meaning. Laurence Erlbaum, 2006.

[36] A. Bunse-Gerstner, R. Byers, and V. Mehrmann. Numerical methods for simultaneous diagonalization.
SIAM Journal on Matrix Analysis and Applications, 14(4):927–949, 1993.

[37] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and T. Telgarsky. Tensor decompositions for learning
latent variable models, 2012. arXiv:1210.7559.

9


