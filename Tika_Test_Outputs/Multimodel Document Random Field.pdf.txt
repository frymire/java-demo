
Learning Cross-modality Similarity for Multinomial Data

Yangqing Jia
UC Berkeley EECS

jiayq@eecs.berkeley.edu

Mathieu Salzmann
TTI-Chicago

salzmann@ttic.edu

Trevor Darrell
UC Berkeley EECS

trevor@eecs.berkeley.edu

Abstract

Many applications involve multiple-modalities such as
text and images that describe the problem of interest. In
order to leverage the information present in all the modali-
ties, one must model the relationships between them. While
some techniques have been proposed to tackle this prob-
lem, they either are restricted to words describing visual
objects only, or require full correspondences between the
different modalities. As a consequence, they are unable to
tackle more realistic scenarios where a narrative text is only
loosely related to an image, and where only a few image-text
pairs are available. In this paper, we propose a model that
addresses both these challenges. Our model can be seen
as a Markov random field of topic models, which connects
the documents based on their similarity. As a consequence,
the topics learned with our model are shared across con-
nected documents, thus encoding the relations between dif-
ferent modalities. We demonstrate the effectiveness of our
model for image retrieval from a loosely related text.

1. Introduction
Many real-world applications involve multi-modal data,

where information arises from different sources, such as im-
ages, text, or speech. In this paper, we focus on images with
loosely related narrative text descriptions, which are a nat-
ural way of providing rich information about the image, not
restricted to exploiting words associated to visible objects.
Figure 1 gives an example of this: Objects irrelevant to the
description of the image, such as sky and cranes, are not
present in the text, while non-visual words, such as launch,
maiden flight and accomplish, strongly help understanding
the image at a high level. Even though existing techniques
have tackled the problem of leveraging text associated with
images, they typically assume the text to contain mostly
words describing visible objects. As a consequence, they
are not able to exploit the entire information present in a
narrative text.

Combining multiple sources of information can be traced
back to multiple-kernel learning [9]. Recently, fusing text

“A timed exposure of the first Space 
Shuttle mission, STS-1, at Launch 
Pad A, Complex 39, turns the space 
vehicle and support facilities into a 
night-time fantasy of light. To the left 
of the Shuttle are the fixed and the 
rotating service structures.”

Figure 1. Example of an image with a loosely related, narrative
description from wikipedia.

and image information has received much attention. Several
approaches [1, 3, 22, 20, 2] have proposed general proba-
bilistic models to tackle the multi-modal scenario for tasks
such as object detection, recognition, and scene understand-
ing. However, these approaches are restricted to using only
words matching visual objects in the images. These words
typically correspond to category labels, or tags, and richer
information in the text is discarded.

In the text processing community, topic models, such as
Latent Dirichlet Allocation (LDA) [5], have proved effec-
tive at discovering the underlying topics in text documents,
and thus at modeling more than single words. To this end,
they learn the groups of semantically consistent words that
generate the training data. Topic models were extended to
the image domain by replacing text words with local image
descriptors [19, 18]. The resulting models have been suc-
cessfully applied to problems such as scene classification
and content-based image retrieval. Modeling spatial inter-
actions across topics in an LDA model has recently been
addressed for image segmentation [23] by defining a spatial
graph over the topic activations of local image patches.

While LDA is effective in these single modality scenar-
ios, it does not directly apply to the multi-modal case. In
particular, LDA does not provide a mechanism to model the
relationships between topics coming from different modal-
ities. To address this issue, other models have been devel-
oped. For instance, Correspondence LDA (Corr-LDA) [3]
was proposed to capture the topic-level relations between
images and text annotations. Corr-LDA assumes a one-to-
one correspondence between the topics of each modality. In
other words, each image topic must have a corresponding
text topic. To generalize over this, a topic regression multi-



modal LDA was recently proposed [13]. This model learns
a regression from the topics in one modality to those in the
other. As a result, it does not have a one-to-one correspon-
dence between each individual topic, but between the sets
of topics describing each modality in a document. Unfortu-
nately, this still assumes that each image is associated with
a text description. Furthermore, in practice, these types of
models have only been applied to the case where all the
words in the description have a visual interpretation. In
more realistic scenarios where images and text are loosely
related, these models would therefore neglect most of the
text information.

In this paper, we introduce a model that addresses the
two above-mentioned issues. In particular, our model is
able to leverage the information of non-visual words in a
text loosely related to an image. Furthermore, we do not
require to be given pairs of corresponding image and text,
but only employ the notion of similarities between two doc-
uments containing a single modality. As a consequence, our
model can exploit the availability of only a few image-text
pairs, together with image-image and text-text similarities,
to learn the intrinsic relationships between images and text.

More specifically, our model can be seen as a Markov
random field (MRF) over LDA topic models. Each node of
the MRF represents the topic model for a particular docu-
ment, which contains a single modality. The edges in the
graph encode the similarity between two documents con-
taining either the same modality, or different ones. Each
document is then generated not only from its own topics,
but also from the topics of the documents connected to it.
Learning our model therefore yields topics that are shared
across several documents. As a consequence, when two
linked documents contain different modalities, our model
learns the relations between these modalities. We name our
model Multi-modal Document Random Field (MDRF).

We demonstrate the benefit of our approach over existing
multi-modal LDA models on the task of retrieving images
from loosely related text descriptions.

2. Modeling Multi-modal Data
In this section, we first briefly review LDA and its multi-

modal extensions, and then explain the generative process
of our model.

2.1. LDA and Corr-LDA Revisited

Latent Dirichlet Allocation [5] is a generative probabilis-
tic model for collections of discrete data. In general, LDA
aims to discover the topics that generate the documents in a
corpus, as well as the topic proportion for each document.
More specifically, following the notation in Figure 2, the
topic proportion θd for a particular document d follows a
Dirichlet distribution with parameter α. Given θd, a par-
ticular topic zdn is drawn from a multinomial distribution,

α

θd

zdn

wdn

N

D

K

β

φk

α

θd

zdn

wdn

N

D

K

β

φk
K

w�
dn� φ�

k

β�

N �

ydn�

Figure 2. Graphical models of LDA (left) and Corr-LDA (right).

and in turn, a word wdn from the corresponding topic-word
multinomial distribution φk, which is drawn from a Dirich-
let distribution with prior β. This defines the marginal prob-
ability for a document as

p(wd|α, β) =

∫
p(θd|α) (1)

×

(
N∏

n=1

∑
zdn

p(zdn|θd)p(wdn|zdn, β)

)
dθd ,

The probability distribution for the whole document corpus
is taken as the product of the probability of each document.

Correspondence LDA [3] was introduced to account for
the availability of multiple modalities in the LDA frame-
work. In particular, it tackles the problem of modeling an-
notated images. The image part is modeled using standard
LDA. To generate the text, a region indicator ydn′ is drawn
from a uniform distribution over {1, · · · , N}, and used in
conjunction with the image topic assignment zdn to draw
the text words w′dn from a multinomial distribution with
dirichlet prior β′. From this, it can be seen that Corr-LDA
treats the two modalities differently: The text topics are
sampled from the empirical distribution of the image top-
ics. Thus if a topic is not discovered from the images, this
topic won’t be available to generate the text. As mentioned
before, this limits the applicability of Corr-LDA in scenar-
ios where the text is more loosely related to the images.

To generalize over this requirement for one-to-one topic
correspondence, the topic regression multi-modal LDA
model was recently proposed [13]. In essence, this model
learns a linear mapping between the topics proportions for
one modality and the topics proportions for the other. As in
Corr-LDA, the text modality can then be generated from the
topic proportions computed for the image modality. How-
ever, the dependencies between the topics is weaker than
in the Corr-LDA case. Instead of relying on a multinomial
distribution to generate the topics, the model uses a logistic
normal distribution, as the correlated topic model [4]. This
generalizes over the Corr-LDA model, but has the draw-
back of making inference more complicated, since there are
O(K2) additional parameters to learn. More importantly,
this still assumes that image-text pairs are available for all



α

θd

zdn

wdn

N

D

φmk

βm

K
M

yd

...
...

θd�� θd�

Figure 3. The Graphical model of the Multi-modal Document Ran-
dom Field model. The dashed edges denote the similarities be-
tween different documents.

the documents. As described below, our model addresses
this issue by considering the notion of similarities between
two documents, thus having weaker requirements for the
documents available at training.

2.2. Multi-modal Document Random Field Model

Our paper focuses on learning a generative
topic model from a set of documents D =
{(y1,w1), (y2,w2), · · · , (yD,wD)}. Each document
(yd,wd) contains an index yd ∈ {1, 2, · · · ,M} selecting
one modality among M possible ones, and a set wd

of words drawn from the vocabulary of this particular
modality. Without loss of generality, we assume that
each word wdn (1 ≤ n ≤ Nd) takes a discrete value
in {1, 2, · · · , Vm}, where Vm is the vocabulary size of
the m-th modality. Note that as opposed to Corr-LDA
and other existing multi-modal topic models, we do not
assume a full set of corresponding documents across the
different modalities. In other words, we do not assume
that there exists a corresponding text document for each
image document. Instead, we assume that we are given a
document-level similarity graph G = (D, E), where E is a
set of edges modeling the similarity between different doc-
uments. If there is an edge e = (i, j) between document i
and document j, the two documents are considered similar.
Note that this is a weaker requirement than one-to-one
correspondences, since the graph might not contain all
image-text pairs, and allows for more general similarities,
such as image-image ones. As we show below, this serves
as a weakly-supervised information to help us discover the
topics shared across documents and modalities.

Figure 3 depicts the graphical model of our approach,
where α and β1...M are the hyperparameters for the Dirich-
let priors. In this graphical model, each document is repre-
sented with an LDA model. In addition to this, we model
the relationships between pairs of documents with the sim-
ilarity graph G. This graph defines a Markov random field

over the documents. For each edge e = (i, j) in the graph,
we define the potential function

ψ(θi,θj) = exp (−λf(θi,θj)) , (2)

where f(θi,θj) is a distance measure between two docu-
ments, and λ is the parameter that controls the peakyness
of the potential function, which can be interpreted as the
strength of the similarity. Several distance measures can be
employed, the simplest of which is the Euclidean distance.
Here, we choose the symmetric KL-divergence defined as

f(θi,θj) =
1

2
(DKL(θi||θj) +DKL(θj ||θi)) (3)

=
1

2

K∑
k=1

(
θik log

θik
θjk

+ θjk log
θjk
θik

)
. (4)

From a generative perspective, each document d in modeled
by first generating a topic distribution θd, and then sampling
the words of that document given θd. Similarly as in LDA,
we generate θd, from a Dirichlet prior. However, in addi-
tion to this prior, the topic distribution also depends on the
random field. More specifically, given the hyperparameters,
the number of topics K, the graph G, and the vocabulary
size Vm for each modality, the generative procedure goes
through the following steps:

1. For each topic k in each modality m, sample the Vm
dimensional word distribution φmk ∼ Dir(φ|βm).

2. Sample the D topic proportions θ1...D from the distri-
bution

p(θ1...D|α,G) =
1

Z
exp(−λ

∑
i,j∈E

f(θi,θj))

D∏
d=1

Dir(θd|α),

where Z is a normalization constant.
3. For each document d, sample its modality yd from a

uniform distribution over {1, · · · ,M}.
4. For each word wdn:

(a) Sample a topic zdn ∼ Multi(z|θd);

(b) Sample a word wdn ∼ Multi(w|φydzdn
).

From this procedure, and by defining Φ as the set of word-
distribution parameters, the joint probability of a document
corpus given similarities between the documents can be
written as

p(D,θ1...D, z1...D,Φ|α, β1...M ,G)

=
1

Z

M∏
m=1

K∏
k=1

Dir(φmk|βk) exp

(
−λ

∑
i,j∈E

f(θi,θj)

)
(5)

×
D∏

d=1

Dir(θd|α)

(
Nd∏
n=1

Multi(zdn|θd)Multi(wdn|φydzdn
)

)
.



(a) Previous Models (b) Our Model

Figure 4. Comparison of existing multi-modal LDA models with
our model. While previous models define documents in a “su-
perdocument” fashion, our model assumes a single modality per
document.

2.3. Relation to Existing Models

In general, our model is a natural extension of LDA to
the multi-modal case. The key contribution of our model
is the document random field, which enables us to capture
the similarities between documents from different modali-
ties. Note that our definition of a document is different from
existing multi-modal LDA models, who define a document
to be a super-document that contains one sub-document
for each modality. As depicted in Fig. 4, defining docu-
ments to be single-modal enables us to utilize those with-
out cross-modality correspondences, or supervised intra-
document similarities. We will show in the experiments that
such flexibility is particularly helpful when correspondence
information is scarce.

The idea of fusing the Markov Random Field and LDA
has been shown in [23]. However, in this approach, a ran-
dom field is built within each document on the topic level,
in order to capture the spatial relationships between topic
assignments. Our model builds the random field on the doc-
ument level instead, and tackles the problem of multi-modal
data and document similarities.

From a different perspective, our model can be seen as
learning a joint latent space for documents containing dif-
ferent modalities. The similarities between documents are
enforced in the joint latent space in a weakly supervised
manner. Learning shared latent spaces across modalities
has been an active research topic in human pose estimation
[17, 6, 16] and image domain transfers [15]. However, the
existing methods focus on dense, real-valued feature spaces
and are typically designed for Gaussian distributions. Our
work, on the other hand, explores the possibility of finding
shared information in the context of integer-valued multi-
nomial distributions.

In the single-modality case, several methods such as the
Hierarchical Dirichlet Process [21] and Pachinko Alloca-
tion [10] have shown that a deeper topic structure may bet-
ter capture the underlying semantics of the corpus. The po-
tential discrepancy between image topics and text topics, as
raised by [13], can be tackled by assuming topic correspon-
dence at a deeper level. While our model uses LDA as the

α

θd

zdn

wdn

N

D

φmk

βm

K
M

yd

θ̂d�

θ̂d��
θ̂d

...

...

Figure 5. Empirical-MDRF for efficient inference.

generative procedure of the data, a deep topic model can be
naturally employed. This will be the topic of future work.

3. Learning the Model
In this section we describe our learning strategy for the

MDRF model. The hidden variables of the model are the
multinomial distribution parameters Φ and the topic as-
signments for all the documents. We assume a symmet-
ric Dirichlet prior for the topic distribution and word distri-
bution, and take β1...M to be identical for all the modal-
ities. Similarly as in LDA, exact inference is in general
intractable. We therefore need to resolve to one of the
usual approximate inference methods, such as variational
inference [5], expectation propagation [11], or Gibbs sam-
pling [7]. Here, we use Gibbs sampling, since it has proved
effective at avoiding local optima, while yielding relatively
simple algorithms.

3.1. Empirical-MDRF for Efficient Inference

The general MDRF model is able to capture the docu-
ment similarities via the random field. However, inference
with this random field is generally difficult as the topic dis-
tributions for multiple documents are coupled. Inspired by
Corr-LDA, instead of enforcing similarity on θds, we in-
troduce an empirical topic distribution θ̂d for each docu-
ment d, and construct the graph on these distributions. This
yields the generative model depicted in Figure 5. We call
this model the Empirical-MDRF and will use it for all the
experiments in this paper.

Specifically, given a set of topic assignments zd in docu-
ment d, the empirical topic distribution θ̂d is computed as

θ̂dk =
n
(d)
dk + α∑K

k=1 n
(d)
dk +Kα

, (6)

where n(d)dk is the number of occurrences of topic k in docu-
ment d. Note that we introduced a smoothness factor in the
computation of θ̂d. This leads to a more robust estimation in



practice, when we need to compare the similarity between
two documents. In fact, θ̂d is the maximum likelihood es-
timate of the underlying multinomial distribution given the
observation zd sampled from the Dirichlet-multinomial dis-
tribution

p(z|α) =

∫
θ

Multi(z|θ)Dir(θ|α)dθ . (7)

The joint distribution of this empirical model is similar to
that of the original MDRF model. However, as we will show
in the next subsection, inference in the empirical MDRF
model can be performed via an efficient collapsed Gibbs
sampling algorithm.

3.2. Gibbs Sampling

For an excellent discussion about Gibbs sampling for
LDA-like probabilistic models, we refer the reader to [8].
In this paper, we employ a collapsed Gibbs sampling algo-
rithm. To this end, we marginalize out θ and Φ, and only
perform Gibbs sampling on the zs. More specifically, we
sample a topic assignment for one word based on its con-
ditional probability given the observations and the topic as-
signments for the other words, and by integrating out the
multinomial distributions with parameters θ and Φ. For
document d containing modality yd = m, the probability
of the topic assignment of word w being k given the corpus
D, the parameters α and β, and the topic assignments for
the other words z−w is expressed as

P (z = k|D, z−w, α, β) ∝

n
(d)
dk + α∑K

k=1 n
(d)
dk +Kα

×
n
(m)
kw + βy∑Vm

w=1 n
(m)
kw + Vmβm

(8)

×
∏

d′,(d,d′)∈E

exp
(
λf(θ̂d,−z, θ̂d′)− λf(θ̂d,z=k, θ̂d′)

)
,

where n(m)
kw is the number of occurrences of wordw in topic

k for modality m, both excluding the current word. θ̂d,−z
is the empirical topic distribution for document d excluding
the current word, and θ̂d,z=k is the empirical topic distribu-
tion for document dwhen the topic for the current word is k.
The first two terms in this equation are identical to those in
LDA, and the last term encodes the conditional probability
introduced by the random field.

3.3. Parameter Estimation
For all the topic models, determining the hyperparam-

eters of the Dirichlet distributions is an important issue.
While empirically optimal parameter settings are available
for LDA [7] when applied to text processing, such parame-
ter settings might not be optimal for other modalities such
as images. Finding the optimal parameters for our method
by performing a grid-search is also prohibitive. Therefore,

“world, species, united, states, found, north, american,
image, convert, common, large, long, located, city, war,
native, small, family, century, largest, national, water,
time, light, river, plant, popular, designed”

Figure 6. Representative subset of the images in the POTD dataset,
and of the words that appear most frequently in the text corpus.

we seek to automatically learn these parameters from the
training data. This has been shown to be possible when
the latent topic assignments are fixed (i.e., in a slice dur-
ing the Gibbs sampling procedure) [12]. For instance, for
fixed latent variables, the hyperparameter α is obtained by
iteratively carrying out the update rule

α←
α
[(∑D

d=1

∑K
k=1 Ψ(n

(d)
dk + α)

)
−DKΨ(α)

]
K
[(∑D

d=1 Ψ(
∑K

k=1(n
(d)
dk + α))

)
−DΨ(Kα)

] , (9)

where Ψ(·) is the digamma function Ψ(x) = d
dx ln Γ(x).

The other parameters, β1...M , are updated in a similar fash-
ion. In practice, hyperparameter update is performed every
few Gibbs sampling steps. To prevent over-fitting to the
current slice, we only run a limited number of iterations for
each update (1 in our experiments).

4. Experiments
In this section, we empirically show the effectiveness of

our model on the task of multi-modal image retrieval. Since
most existing multi-modal datasets are limited to annota-
tions that describe visible object names only, we collected
a new dataset containing richer and looser text descriptions
of the images. We first describe this dataset and the exper-
imental settings, and then present our results and compare
them against those obtained with LDA and Corr-LDA.

4.1. The Wikipedia POTD Dataset

The wikipedia “Picture of the day” website1 provides a
collection of daily featured pictures. Together with the im-
ages, a short paragraph of about 100 words gives a brief and

1http://en.wikipedia.org/wiki/Wikipedia:Picture of the day



0 0.2 0.4 0.6 0.8 1
0

0.2

0.4

0.6

0.8

1

percentage

e
rr

o
r 

ra
te

 

 

LDA−CCA

LDA−NN

CorrLDA

MDRF

Figure 7. Average error rate as a function of the percentage of the
ranked list considered for retrieval. Curves closer to the axes rep-
resents better performance. See the text for more details.

Method AUC value
LDA-NN 43.15± 1.95
LDA-CCA 39.44± 2.27
Corr-LDA 26.94± 1.87
MDRF 23.14± 1.49

Table 1. Average area under the curve (AUC) (in percentage) and
standard deviations for the curves in Figure 7. A smaller value
indicates a better performance.

loose description of the picture. Figure 6 shows several rep-
resentative images and words from the dataset. Note that
both the pictures and the descriptions cover a wide variety
of topics ranging from celestial pictures to historical pho-
tos. Furthermore, the words are beyond the scope of simple
visual objects present in the images.

For our experiment, we collected the daily pictures and
their corresponding descriptions from Nov 1, 2004 to Oct
30, 2010. After removing non-image data (e.g., movie files)
and text that could not be parsed, we obtained a total of
1987 image-text pairs. We used rainbow2 to tokenize the
text and kept the words that appeared more than 3 times
in the whole corpus. This resulted in a vocabulary of 3562
words. For the images, we computed densely sampled SIFT
features over 16× 16 grids. Each image was resized so that
approximately 400 features were sampled per image. We
randomly chose a subset of 50,000 SIFT features and ran k-
means to obtain 1,000 clusters. These clusters were used
to vector-quantize the SIFT features, thus yielding 1,000
discrete visual words. The dataset can be downloaded at
http://www.eecs.berkeley.edu/∼jiayq/wikipedia potd/.

2http://www.cs.cmu.edu/∼mccallum/bow/rainbow/

Method Percentage
LDA-NN 30.10
LDA-CCA 30.98
Corr-LDA 53.30
MDRF 58.84

Table 2. Percentage of images correctly retrieved in the first 20%
of the ranked list.

4.2. Retrieval Protocol

To test our model and to compare it against existing
methods, we consider the problem of multi-modal image
retrieval. More specifically, given a text query, we aim to
find images that are most relevant to it. For each text in the
test set, we rank the test images using either our approach,
or a competing method. To this end, for Corr-LDA and for
our method, we learn the topic distributions θi for each test
image. Given a text query w = w1, w2, · · · , wN , the score
for each image is then defined as

si = p(w|θi) =

N∏
n=1

p(wn|θi) . (10)

Note that the marginal probabilities p(wn|θi) for all words
can be pre-computed for each image during learning time,
so no marginalization is necessary during query time. An
alternative to this would be to compute the text-topic dis-
tribution and measure the KL-divergence between this dis-
tribution and the image-topic distribution. However, this
requires an inference step for each query, which is time-
consuming. Instead, the score described above is determin-
istic and can be performed in O(N) time.

Since there is only one ground-truth match for each im-
age/text, to evaluate the performance we rely on the position
of the ground-truth image in the ranked list obtained. More
specifically, an image is considered correctly retrieved if it
appears in the first t percent of the list created from its corre-
sponding text. Sweeping through all the text queries gives
us an error rate that is dependent on t, which is shown in
Figure 7.

To obtain statistically valid error measures, we split the
data into 10 folds, and test on each fold with the remaining
9 as training data. For LDA and Corr-LDA, all the hyper-
parameters can be learned directly from the training data
as described in Section 3.3. Our method uses an additional
parameter λ for the document random field. To set this pa-
rameter, we performed a grid-search using cross validation
on the first 9 folds. The optimal value for λ was kept un-
changed for all the other partitions. For all the methods, we
fixed the number of topics to 64. This number was found to
work best for LDA and Corr-LDA, while our method was
not significantly affected by the number of topics. We set
the burn-in period for Gibbs sampling to 1,000 iterations.



“A Hansom cab is a kind of horse-drawn
carriage first designed and patented in 
1834 by Joseph Hansom, an architect

from Leicestershire, England. Its purpose
was to combine speed with safety, with a 

low center of gravity that was essential 
for safe cornering. The Hansom Cab was 

introduced to the United States during 
the late 19th century, and was most 

commonly used there in New York City.”

The night skyline of Frankfurt, showing
the Commerzbank Tower (centre) and the 
Maintower (right of centre). Frankfurt is 
the fifth-largest city in Germany, and the 

surrounding Frankfurt Rhein-Main 
Region is Germany's second-largest

metropolitan area.

“A barn at the Grand Teton National 
Park. The United States National Park, 
named after Grand Teton of the Teton 

Range, is located in western Wyoming, 
south of Yellowstone. The park is located

in the heart of the Greater Yellowstone
Ecosystem, one of the largest intact 

temperate zone ecosystems remaining on 
the planet.”

Thursday, November 11, 2010

Figure 8. Three typical image retrieval results. For each example, we show the query text, the top 5 images returned by our algorithm
(top row), and the top 5 images returned by Corr-LDA (bottom row). The words that are in the vocabulary are colored in blue. For space
consideration, the results of the LDA baselines are not shown here.

We compare our method against Corr-LDA and two
LDA-based baselines3. In the two latter cases, LDA mod-
els are trained separately for images and text. Retrieval is
then performed using either nearest-neighbors (LDA-NN),
or CCA (LDA-CCA) [14]. For LDA-NN, we compute the
nearest neighbor of the query text among the training texts,
take the corresponding training image, and build the ranked
list of test images using the symmetric KL-divergence be-
tween the image topic distributions. LDA-CCA learns the
individual projections of the image and text topic distribu-
tions to a joint latent space in which the correlation between
those distributions is maximum. The ranked list is then ob-
tained from the distances between the test images and the
query text in this latent space. For each experiment, we
searched for the dimensionality of the CCA latent space that

3No reference implementation of the topic regression MMLDA [13] is
available. We implemented a Gibbs sampling version of the algorithm,
which performed worse than Corr-LDA. Since our implementation might
be different from the original one that uses variational inference, we do not
report its performance here. A potential explanation is that topic regression
MMLDA has a large number of parameters to learn, making it less robust
on small training sets such as ours.

gave the best results.

4.3. Results

We now present our results on the POTD dataset. Figure
7 depicts the retrieval errors averaged over the 10 partitions
for all the methods. In Table 1, we report the area under the
curve (AUC) values for those errors. A t-test with thresh-
old 0.01 revealed that the difference between our results and
the others is significant. Since in information retrieval, it is
always valuable to have related documents appear as early
as possible in the ranked list, we also report the percent-
age of the images correctly retrieved in the first 20% of the
ranked list in Table 2. Compared to Corr-LDA, about 5%
more documents on average are accurately retrieved by our
method.

Figure 8 shows several illustrative examples of the re-
trieval results, using text from the POTD pages. Qualita-
tively, it can be observed that our model captures the general
topics represented in both the images and the text better than
Corr-LDA. For instance, in the third query, our model cap-
tures the fact that the national parks mentioned in the text
are closely related to nature and outdoor scenes. In the first



0 0.1 0.2 0.3 0.4 0.5
20

22

24

26

28

30

32

34

missing rate

A
U

C

 

 

MDRF

CorrLDA

Figure 9. Average AUC value as a function of the percentage of
missing correspondences.

query, our model relates the city names in the text to urban
images, whereas Corr-LDA cannot capture this connection,
since city names do not correspond to visible objects in an
image.

Finally, to test the robustness of our algorithm against
missing correspondence information, we removed a sub-
set of the correspondences between images and text when
learning the models. CorrLDA is not able to use the part of
data that do not have correspondence information present,
while our method can process sparse similarity informa-
tion inherently. More specifically, we assume that t per-
cent of the correspondence in the training corpus are un-
known, and vary t from 0 to 50 in our experiments. The
average AUC value versus the proportion of missing corre-
spondences is shown in Figure 9. It can be observed that our
method consistently outperforms CorrLDA. Furthermore,
note that in the limit where no correspondences are avail-
able, Corr-LDA could not be applied at all. In contrast, our
model would still learn topics that generate the documents
well, although they would not necessarily model the cross-
similarities.

5. Conclusion

In this paper, we have proposed a new probabilistic
model that learns cross-modality similarities from a doc-
ument corpus containing multinomial data. While existing
methods require full correspondence between the modali-
ties, our MDRF model defines a Markov random field on
the document level that allows modeling more flexible doc-
ument similarities. As a result, our model learns a set of
shared topics across the modalities. By applying our model
to the task of image retrieval from wikipedia data, where the
narrative text is only loosely related to the images, we have
shown that our method outperforms existing techniques,
which assume the text to contain visual objects only. In
the future, we intend to study the use of deeper topic struc-
tures, such as Pachinko Allocation [10], to better capture
the semantics shared among the documents.

References
[1] K. Barnard, P. Duygulu, D. Forsyth, N. De Freitas, D. Blei,

and M. Jordan. Matching words and pictures. JMLR,
3:1107–1135, 2003. 1

[2] T. Berg, A. Berg, J. Edwards, M. Maire, R. White, Y. Teh,
E. Learned-Miller, and D. Forsyth. Names and faces in the
news. In CVPR, 2004. 1

[3] D. Blei and M. Jordan. Modeling annotated data. In SIGIR,
2003. 1, 2

[4] D. Blei and J. Lafferty. A correlated topic model of science.
Annals of Applied Statistics, 1(1):17–35, 2007. 2

[5] D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation.
JMLR, 3:993–1022, 2003. 1, 2, 4

[6] C. Ek. Shared Gaussian Process Latent Variable Models.
Ph.D. Thesis, 2009. 4

[7] T. Griffiths. Finding scientific topics. Proceedings of the
National Academy of Science, 101:5228–5235, 2004. 4, 5

[8] G. Heinrich. Parameter estimation for text analysis. Techni-
cal Report, 2005. 5

[9] G. Lanckriet, N. Cristianini, P. Bartlett, L. Ghaoui, and
M. Jordan. Learning the kernel matrix with semidefinite pro-
gramming. JMLR, 5:27–72, 2004. 1

[10] W. Li and A. McCallum. Pachinko allocation: DAG-
structured mixture models of topic correlations. In ICML,
2006. 4, 8

[11] T. Minka. Expectation propagation for approximate
Bayesian inference. In UAI, 2001. 4

[12] T. Minka. Estimating a Dirichlet distribution. Technical re-
port, MIT, 2003, 2003. 5

[13] D. Putthividhy, H. Attias, and S. Nagarajan. Topic regres-
sion multi-modal Latent Dirichlet Allocation for image an-
notation. In CVPR, 2010. 2, 4, 7

[14] N. Rasiwasia, J. Pereira, E. Coviello, G. Doyle, G. Lanckriet,
R. Levy, and N. Vasconcelos. A New Approach to Cross-
Modal Multimedia Retrieval. In ACM MM, 2010. 7

[15] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting vi-
sual category models to new domains. In ECCV, 2010. 4

[16] M. Salzmann, C. Ek, R. Urtasun, and T. Darrell. Factorized
orthogonal latent spaces. In AISTATS, 2010. 4

[17] A. Shon, K. Grochow, A. Hertzmann, and R. Rao. Learn-
ing shared latent structure for image synthesis and robotic
imitation. In NIPS, 2005. 4

[18] J. Sivic, B. Russell, A. Efros, A. Zisserman, and W. Freeman.
Discovering object categories in image collections. In ICCV,
2005. 1

[19] J. Sivic and A. Zisserman. Video Google: A text retrieval
approach to object matching in videos. In ICCV, 2003. 1

[20] R. Socher and L. Fei-Fei. Connecting modalities: Semi-
supervised segmentation and annotation of images using un-
aligned text corpora. In CVPR, 2010. 1

[21] Y. Teh, M. Jordan, M. Beal, and D. Blei. Hierarchical dirich-
let processes. Journal of the American Statistical Associa-
tion, 101(476):1566–1581, 2006. 4

[22] C. Wang, D. Blei, and F. Li. Simultaneous image classifica-
tion and annotation. In CVPR, 2009. 1

[23] B. Zhao, L. Fei-Fei, and E. Xing. Image Segmentation with
Topic Random Fields. In ECCV, 2010. 1, 4


